{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game: Just one\n",
    "\n",
    "The idea is to play the guesser in the popular game Just One. Given many words associated with the word to guess (no synonyms), the guesser must find the most plausible word. We use word embeddings to find the average vector between all provided words and give as an answer the embedding most strongly correlated with this average vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52343\n",
      "128261\n",
      "151102\n",
      "200668\n",
      "209833\n",
      "220779\n",
      "253461\n",
      "365745\n",
      "532048\n",
      "717302\n",
      "994818\n",
      "1123331\n",
      "1148409\n",
      "1352110\n",
      "1499727\n",
      "1533809\n",
      "1899841\n",
      "1921152\n",
      "2058966\n",
      "2165246\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.840B.300d.txt\", 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "        except:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some words have embeddings of length 299, identify the bad keys here\n",
    "bad_keys = []\n",
    "for word in embeddings_dict.keys():\n",
    "    if len(embeddings_dict[word])!=300:\n",
    "        bad_keys.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the bad keys\n",
    "for key in bad_keys:\n",
    "    del embeddings_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.cosine(embeddings_dict[word], embedding))\n",
    "#alternative spatial.distance.euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_words = ['fire', 'amazon', 'brazil', 'wet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_embeddings = np.asarray([embeddings_dict[word] for word in given_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1392    0.10877   0.42009  ...  0.030956 -0.24704   0.056905]\n",
      " [-0.73095   0.45252   0.1357   ... -0.24908  -0.091376  0.077178]\n",
      " [-0.067526  0.050342  0.59258  ... -0.59192  -0.15179   0.29805 ]\n",
      " [ 0.29371  -0.37684   0.014916 ...  0.12695   0.16432  -0.5478  ]]\n"
     ]
    }
   ],
   "source": [
    "print(given_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_embeddings_mean = np.mean(given_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 200 embeddings closest to the given mean\n",
    "closest_to_mean = find_closest_embeddings(given_embeddings_mean)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words from result which are identical to the given words\n",
    "closest_to_mean = [word for word in closest_to_mean if word not in given_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words from result which if stemmed are identical to the given words\n",
    "ps = PorterStemmer()\n",
    "closest_to_mean = [word for word in closest_to_mean if ps.stem(word) not in given_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words from result which are synonyms of the given words\n",
    "synonyms = []\n",
    "for gword in given_words:\n",
    "    for syn in wordnet.synsets(gword):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "\n",
    "closest_to_mean = [word for word in closest_to_mean if word not in synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brazilian', 'hot', 'america', 'ass', 'asian', 'rain', 'africa', 'african', 'naked', 'butt', 'pussy', 'blow', 'asia', 'american', 'bush', 'dirty', 'australia', 'fuck', 'latina', 'butts', 'ebony', 'babes', 'fucking', 'japan', 'big', 'burning', 'japanese', 'europe', 'indian', 'suck', 'dick', 'porn', 'australian', 'european', 'bbw', 'forest', 'water', 'latin', 'virgin', 'tits', 'booty', 'uk', 'licking', 'lick', 'british', 'dry', 'girl', 'anal', 'dvd', 'chicago', 'asses', 'babe', 'canada', 'nude', 'cock', 'argentina', 'sucking', 'jungle', 'usa', 'black', 'anderson', 'walmart', 'myspace', 'smoke', 'bare', 'dildo', 'earth', 'lesbian', 'mexican', 'hairy', 'teen', 'shemale', 'wild', 'bikini', 'piss', 'england', 'pissing', 'germany', 'fucked', 'sun', 'sex', 'india', 'sexy', 'thunder', 'lightning', 'xxx', 'brown', 'blonde', 'kiss', 'busty', 'boobs', 'dicks', 'cold', 'blowing', 'london', 'chick', 'sand', 'snow', 'kelly', 'britney', 'bang', 'tgp', 'nasty', 'sky', 'mexico', 'huge', 'hell', 'russian', 'cunt', 'thai', 'shower', 'palm', 'storm', 'hard', 'pee', 'christmas', 'ash', 'woods', 'fist', 'crazy', 'chicks', 'florida', 'blowjob', 'wood', 'panties', 'cant', 'john', 'pic', 'taylor', 'dark', 'russia', 'horny', 'air', 'dripping', 'girls', 'i', 'outdoors', 'banging', 'zealand', 'free', 'pics', 'rio', 'flames', 'french', 'orgasm', 'milf', 'jennifer', 'rainforest', 'dutch', 'maria', 'pacific', 'thick', 'hawaii', 'lady', 'blows', 'fox', 'mud', 'off', 'red', 'squirt', 'heat', 'rough', 'california', 'santa', 'bitch', 'china', 'tit', 'moist', 'beach', 'miami', 'clothes', 'chubby', 'yahoo', 'squirting', 'diego', 'ground', 'redhead', 'moore', 'thailand', 'kissing', 'chinese', 'green', 'theres', 'boy', 'spears', 'italy', 'peter', 'sweat', 'canadian', 'fetish', 'wikipedia', 'fisting']\n"
     ]
    }
   ],
   "source": [
    "print(closest_to_mean[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piano', 'instruments', 'violin', 'guitar', 'melody', 'keyboard', 'orchestra', 'musical', 'flute', 'sound', 'instrumental', 'tunes', 'cello', 'symphony', 'midi', 'songs', 'saxophone', 'clarinet', 'orchestral', 'melodies']\n"
     ]
    }
   ],
   "source": [
    "# result for ['keys', 'music', 'instrument', 'mozart']\n",
    "print(closest_to_mean[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shop', 'furnishings', 'stores', 'decor', 'kitchen', 'wood', 'antique', 'chairs', 'wooden', 'cabinets', 'clothing', 'clothes', 'warehouse', 'decorating', 'furnishing', 'sofa', 'shelves', 'ikea', 'danish', 'wicker']\n"
     ]
    }
   ],
   "source": [
    "# result for ['furniture', 'store', 'swedish', 'assemble']\n",
    "print(closest_to_mean[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
